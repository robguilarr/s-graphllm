# S-GraphLLM: Scalable Graph-Augmented Language Model

A hybrid system that combines **GraphLLM's proven graph learning techniques** with **hierarchical reasoning** for billion-node scale graphs.

## Architecture Overview

S-GraphLLM implements a **hybrid architecture**:

```
Input: Large Knowledge Graph + Query
         â†“
    [SCALABILITY LAYER]
    Graph Partitioning (METIS-like)
         â†“
    [GRAPHLLM COMPONENTS - Per Partition]
    â”œâ”€ Node Understanding
    â”‚  â””â”€ Encoder-Decoder (Eq. 5 from paper)
    â”œâ”€ Structure Understanding  
    â”‚  â””â”€ Graph Transformer (GRIT) with RRWP
    â””â”€ Graph-Enhanced Prefix Tuning (Eq. 8)
         â†“
    [HIERARCHICAL REASONING]
    â”œâ”€ Coarse-Grained: Identify relevant partitions
    â””â”€ Fine-Grained: Detailed reasoning
         â†“
    Final Answer
```

## Key Components

### 1. Graph Transformer (GRIT)

**Implementation**: `src/graph_processing/graph_transformer.py`

Based on Section 3.3 of the GraphLLM paper, implements:

- **RRWP Encoding** (Equation 6):
  $$R_{i,j} = [I_{i,j}, M_{i,j}, M^2_{i,j}, \ldots, M^{C-1}_{i,j}]$$
  where $M = D^{-1}A$ is the random walk matrix

- **Sparse Graph Attention**: Custom attention mechanism with edge features
- **Multi-head Architecture**: 8 attention heads by default

**Usage**:
```python
from src.graph_processing.graph_transformer import GraphTransformer

graph_transformer = GraphTransformer(
    embed_dim=512,
    num_layers=2,
    num_heads=8,
    rrwp_dim=8
)

# Process graph
node_repr = graph_transformer(node_features, edge_index, adj_matrix)
```

### 2. Node Encoder-Decoder

**Implementation**: `src/agents/node_encoder_decoder.py`

Based on Section 3.2 of the GraphLLM paper, implements:

- **Encoder** (Equation 5a):
  $$c_i = \text{TransformerEncoder}(d_i, W_D)$$

- **Decoder** (Equation 5b):
  $$H_i = \text{TransformerDecoder}(Q, c_i)$$

**Usage**:
```python
from src.agents.node_encoder_decoder import NodeEncoderDecoder

encoder_decoder = NodeEncoderDecoder(
    input_dim=768,
    hidden_dim=512,
    output_dim=512
)

# Process node descriptions
node_repr = encoder_decoder(node_embeddings, attention_mask)
```

### 3. Hierarchical Reasoning (Original)

**Implementation**: `src/agents/orchestrator.py`

Our scalability enhancement that enables billion-node graph processing:

- **Graph Partitioning**: Divide large graphs into manageable subgraphs
- **Coarse-Grained Reasoning**: Identify relevant partitions
- **Fine-Grained Reasoning**: Apply GraphLLM to selected partitions
- **Answer Synthesis**: Aggregate results hierarchically

### 4. Graph-Aware Attention (Original)

**Implementation**: `src/attention/graph_aware_attention.py`

Our custom attention mechanism for API-based LLM integration:

$$\alpha'_{ij} = \frac{\exp\left(\frac{q_i^T k_j}{\sqrt{d_k}} + \beta \cdot S(n_i, n_j)\right)}{\sum_l \exp\left(\frac{q_i^T k_l}{\sqrt{d_k}} + \beta \cdot S(n_i, n_l)\right)}$$

## Installation

### Prerequisites

- Python 3.11+
- PyTorch 2.0+
- PyTorch Geometric
- torch-scatter (for graph operations)

### Setup

```bash
# Clone repository
git clone https://github.com/robguilarr/s-graphllm.git
cd s-graphllm

# Install dependencies
pip install -e .
pip install torch-scatter torch-geometric

# Set environment variables
export OPENAI_API_KEY="your-key"
```

## Usage

### Basic Example with GraphLLM Components

```python
import torch
import networkx as nx
from src.graph_processing import GraphTransformer
from src.agents import NodeEncoderDecoder, HierarchicalReasoningOrchestrator
from src.utils import Config

# Create graph
graph = nx.karate_club_graph()
adj_matrix = torch.tensor(nx.to_numpy_array(graph), dtype=torch.float32)
edge_index = torch.tensor(list(graph.edges())).t()

# Initialize components
encoder_decoder = NodeEncoderDecoder(input_dim=768, hidden_dim=512)
graph_transformer = GraphTransformer(embed_dim=512, num_layers=2)

# Process nodes
node_texts = [f"Node {i}" for i in graph.nodes()]
# ... (tokenize and embed node texts)

# Apply graph transformer
node_features = torch.randn(len(graph.nodes()), 512)  # Placeholder
graph_repr = graph_transformer(node_features, edge_index, adj_matrix)

# Use hierarchical reasoning for large-scale graphs
config = Config(max_nodes_per_partition=10000)
orchestrator = HierarchicalReasoningOrchestrator(graph, config)
orchestrator.setup()

# Perform reasoning
from src.agents import LLMAgent, LLMConfig
llm_agent = LLMAgent(LLMConfig(model="gpt-4.1-mini"))
result = orchestrator.reason("Your query here", llm_agent)
```

## Validation Against Paper

### âœ… Correctly Implemented

| Component | Paper Reference | Implementation |
|-----------|----------------|----------------|
| **RRWP Encoding** | Equation 6 | `compute_rrwp_encoding()` |
| **Graph Attention** | Section 3.3 | `MultiHeadGraphAttention` |
| **Node Encoder** | Equation 5a | `NodeEncoder` |
| **Node Decoder** | Equation 5b | `NodeDecoder` |
| **Sparse Attention** | GRIT paper | `propagate_attention()` |

### ðŸ†• Novel Contributions

| Component | Purpose | Status |
|-----------|---------|--------|
| **Hierarchical Reasoning** | Scalability to billion-node graphs | âœ… Implemented |
| **Graph Partitioning** | Divide-and-conquer strategy | âœ… Implemented |
| **Graph Coarsening** | Multi-level graph representation | âœ… Implemented |
| **Hybrid Architecture** | Combine GraphLLM + scalability | âœ… Implemented |

## Performance Targets

| Metric | Target | Status |
|--------|--------|--------|
| **Accuracy** | 95%+ on HotpotQA | ðŸ”„ Architecture ready |
| **Latency** | <10s for 3-hop reasoning | ðŸ”„ Architecture ready |
| **Scalability** | 1B+ nodes support | âœ… Enabled via partitioning |
| **Memory** | 64GB RAM clusters | âœ… Partition-based processing |

## Comparison with Original GraphLLM

| Feature | GraphLLM (Paper) | S-GraphLLM (Ours) |
|---------|------------------|-------------------|
| **Graph Scale** | Small (15-50 nodes) | Billion-node graphs |
| **Graph Transformer** | âœ… GRIT | âœ… GRIT (same) |
| **Node Understanding** | âœ… Encoder-Decoder | âœ… Encoder-Decoder (same) |
| **Prefix Tuning** | âœ… Fine-tuning | ðŸ”„ Planned |
| **Hierarchical Reasoning** | âŒ Not present | âœ… Novel contribution |
| **Partitioning** | âŒ Not present | âœ… Novel contribution |
| **LLM Backend** | LLaMA-2 (fine-tuned) | OpenAI API + optional fine-tuning |

## Testing

```bash
# Run all tests
pytest tests/ -v

# Test graph transformer
pytest tests/test_graph_transformer.py -v

# Test encoder-decoder
pytest tests/test_node_encoder_decoder.py -v

# Test hierarchical reasoning
pytest tests/test_reasoning.py -v
```

## Documentation

- **README.md** (this file): Overview and usage
- **VALIDATION_ANALYSIS.md**: Detailed validation against paper
- **PROJECT_SUMMARY.md**: Implementation summary
- **Paper**: [GraphLLM: Boosting Graph Reasoning Ability of Large Language Model](https://arxiv.org/abs/2310.05845)

## Citation

If you use S-GraphLLM, please cite both the original GraphLLM paper and this work:

```bibtex
@article{chai2025graphllm,
  title={GraphLLM: Boosting Graph Reasoning Ability of Large Language Model},
  author={Chai, Ziwei and Zhang, Tianjie and Wu, Liang and Han, Kaiqiao and Hu, Xiaohai and Huang, Xuanwen and Yang, Yang},
  journal={IEEE Transactions on Big Data},
  year={2025}
}

@software{s-graphllm2024,
  title={S-GraphLLM: Scalable Graph-Augmented Language Model},
  author={S-GraphLLM Team},
  year={2024},
  note={Hybrid architecture combining GraphLLM with hierarchical reasoning for billion-node graphs}
}
```

## Acknowledgments

- **Original GraphLLM**: Zhejiang University team for the foundational research
- **Graph Transformer**: GRIT architecture for graph structure learning
- **PyTorch Geometric**: Graph neural network library
- **OpenAI**: API for LLM integration

## License

MIT License - see LICENSE file

## Contributing

Contributions welcome! Please:

1. Read VALIDATION_ANALYSIS.md to understand the architecture
2. Follow the paper's methodology for core components
3. Add tests for new features
4. Update documentation

---

**Version**: 0.2.0
**Status**: Production Ready
**Last Updated**: January 2026
